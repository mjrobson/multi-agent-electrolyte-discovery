{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16194a50",
   "metadata": {},
   "source": [
    "# Run Multi-Agent Network model\n",
    "\n",
    "This notebook guides you through running the multi-agent network model.\n",
    "\n",
    "    Note: If you haven't already completed the setup, please run one-time-setup.ipynb first. It configures the required vector databases.\n",
    "\n",
    "This notebook will read from existing vector databases and then initialize and execute the multi-agent network. It does not modify or rebuild the databases.\n",
    "\n",
    "___________________________________________\n",
    "\n",
    "## 1. Set Up API Keys\n",
    "\n",
    "As with the setup notebook, use the cell below to enter your OpenAI API key. This will be stored as an environment variable for the current session only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353a5b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = ''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c937cb",
   "metadata": {},
   "source": [
    "## 2. Read RAG databases\n",
    "\n",
    "This cell loads the Chroma vector databases containing academic papers and their corresponding summaries, which were generated during `one-time-setup.ipynb`.\n",
    "\n",
    "The locations of the persistent databases are defined using the `PAPER_PERSIST_DIR` and `SUMMARY_PERSIST_DIR` variables, specified as relative paths.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244d8b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Load Prebuilt Vector Databases === #\n",
    "\n",
    "from model_resources.functions.vector_db import read_text_database\n",
    "\n",
    "# Define relative paths to persisted Chroma vector databases\n",
    "PAPER_PERSIST_DIR = \"./model_databases/paperstore\"\n",
    "SUMMARY_PERSIST_DIR = \"./model_databases/summarystore\"\n",
    "\n",
    "# Initialize the paper and summary vector stores\n",
    "paperdb = read_text_database(PAPER_PERSIST_DIR)\n",
    "summarydb = read_text_database(SUMMARY_PERSIST_DIR)\n",
    "\n",
    "# Convert vector stores into retrievers for use in the RAG pipeline\n",
    "paper_retriever = paperdb.as_retriever()\n",
    "summary_retriever = summarydb.as_retriever()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd2ede4",
   "metadata": {},
   "source": [
    "## 3. Set Up Retriever Tools\n",
    "\n",
    "This section initializes the Paper and Summary Retriever Tools used by the multi-agent network.\n",
    "\n",
    "\n",
    "These retrievers are wrapped as tools using LangChain's create_retriever_tool, and then passed to a ToolExecutor for use in the agent workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbe7a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools.retriever import create_retriever_tool\n",
    "from langgraph.prebuilt import ToolExecutor\n",
    "import model_resources.functions.multi_agent_network as man\n",
    "\n",
    "# === Create Retriever Tools === #\n",
    "\n",
    "paper_tool = create_retriever_tool(\n",
    "    paper_retriever,\n",
    "    \"paper_retriever\",\n",
    "    \"This tool gives access to the academic literature on deep eutectic electrolytes for zinc batteries. \"\n",
    "    \"It can be used to extract detailed performance information about specific electrolyte components.\"\n",
    ")\n",
    "\n",
    "summary_tool = create_retriever_tool(\n",
    "    summary_retriever,\n",
    "    \"summary_retriever\",\n",
    "    \"This tool gives access to summaries that provide an overview of the field of deep eutectic electrolytes for zinc batteries. \"\n",
    "    \"It is useful for determining whether a specific electrolyte composition has been tested in the literature.\"\n",
    ")\n",
    "\n",
    "# === Register Tools with ToolExecutor === #\n",
    "\n",
    "tools = [paper_tool, summary_tool]\n",
    "tool_executor = ToolExecutor(tools)\n",
    "\n",
    "# === Inject ToolExecutor into Multi-Agent Network Module === #\n",
    "\n",
    "man.tool_executor = tool_executor\n",
    "\n",
    "# Expose the tool_node from the module for use in the graph\n",
    "tool_node = man.tool_node"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df138c6c",
   "metadata": {},
   "source": [
    "## 4. Set Up Agent Roles\n",
    "\n",
    "This section creates two agents with distinct roles and capabilities:\n",
    "\n",
    "- Scientist Agent: Uses the `paper_retriever` tool to extract detailed technical and performance information from academic literature.\n",
    "\n",
    "- Principal Investigator (PI) Agent: Uses the `summary_retriever` tool to assess the research landscape, helping determine whether a given electrolyte composition has been previously studied.\n",
    "\n",
    "Each agent is initialized with a custom system prompt (loaded from a text file) and wrapped into a callable node for use in the LangGraph network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bfcaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from model_resources.functions.multi_agent_network import create_agent, agent_node\n",
    "\n",
    "# === Load System Prompts from Files === #\n",
    "\n",
    "def load_agent_message(path: str) -> str:\n",
    "    \"\"\"Utility function to read an agent's system prompt from a file.\"\"\"\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return f.read()\n",
    "\n",
    "SCIENTIST_MSG_PATH = \"model_resources/agent_messages/scientist_message.txt\"\n",
    "PI_MSG_PATH = \"model_resources/agent_messages/PI_message.txt\"\n",
    "\n",
    "scientist_message = load_agent_message(SCIENTIST_MSG_PATH)\n",
    "pi_message = load_agent_message(PI_MSG_PATH)\n",
    "\n",
    "# === Initialize Language Model === #\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4-0125-preview\",\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "# === Create Scientist Agent and Node === #\n",
    "\n",
    "scientist_agent = create_agent(\n",
    "    llm=llm,\n",
    "    tools=[paper_tool],\n",
    "    system_message=scientist_message,\n",
    ")\n",
    "\n",
    "scientist_node = functools.partial(\n",
    "    agent_node,\n",
    "    agent=scientist_agent,\n",
    "    name=\"Scientist\"\n",
    ")\n",
    "\n",
    "# === Create Principal Investigator Agent and Node === #\n",
    "\n",
    "PI_agent = create_agent(\n",
    "    llm=llm,\n",
    "    tools=[summary_tool],\n",
    "    system_message=pi_message,\n",
    ")\n",
    "\n",
    "PI_node = functools.partial(\n",
    "    agent_node,\n",
    "    agent=PI_agent,\n",
    "    name=\"Principal Investigator\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de59400e",
   "metadata": {},
   "source": [
    "## 5. Build the Multi-Agent Workflow Graph\n",
    "\n",
    "This step defines the multi-agent workflow using LangGraph.\n",
    "Agents take turns invoking tools or passing control based on the message content.\n",
    "\n",
    "- The Principal Investigator agent starts the interaction.\n",
    "\n",
    "- Agents alternate based on the router's decision:\n",
    "\n",
    "    - `continue`: passes control to the other agent\n",
    "\n",
    "    - `call_tool`: invokes a tool\n",
    "\n",
    "    - `end`: terminates the graph if a final answer is detected\n",
    "\n",
    "The `tool_node` returns control to the agent who invoked the tool, determined by the sender field in the state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc82f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph\n",
    "from model_resources.functions.multi_agent_network import AgentState, tool_node, router\n",
    "\n",
    "# === Initialize Workflow Graph === #\n",
    "\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# === Add Agent and Tool Nodes === #\n",
    "\n",
    "workflow.add_node(\"Scientist\", scientist_node)\n",
    "workflow.add_node(\"Principal Investigator\", PI_node)\n",
    "workflow.add_node(\"call_tool\", tool_node)\n",
    "\n",
    "# === Define Agent Transition Logic === #\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"Scientist\",\n",
    "    router,\n",
    "    {\n",
    "        \"continue\": \"Principal Investigator\",\n",
    "        \"call_tool\": \"call_tool\",\n",
    "        \"end\": END,\n",
    "    },\n",
    ")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"Principal Investigator\",\n",
    "    router,\n",
    "    {\n",
    "        \"continue\": \"Scientist\",\n",
    "        \"call_tool\": \"call_tool\",\n",
    "        \"end\": END,\n",
    "    },\n",
    ")\n",
    "\n",
    "# === Define Tool Return Routing === #\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"call_tool\",\n",
    "    # Route back to the original sender after the tool finishes\n",
    "    lambda x: x[\"sender\"],\n",
    "    {\n",
    "        \"Scientist\": \"Scientist\",\n",
    "        \"Principal Investigator\": \"Principal Investigator\",\n",
    "    },\n",
    ")\n",
    "\n",
    "# === Set Entry Point and Compile Graph === #\n",
    "\n",
    "workflow.set_entry_point(\"Principal Investigator\")\n",
    "graph = workflow.compile()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41777d8",
   "metadata": {},
   "source": [
    "## 6. Run the Multi-Agent Graph\n",
    "\n",
    "This step streams the conversation through the compiled multi-agent graph using the initial user prompt.\n",
    "Each agent takes turns reasoning, invoking tools, or concluding with a `FINAL ANSWER`.\n",
    "\n",
    "- The input prompt is loaded from a file (`model_resources/prompts/initial_prompt.txt`)\n",
    "\n",
    "- Output is streamed step-by-step to the console and saved to a timestamped .txt file in the results/ directory.\n",
    "\n",
    "- The graph is limited to a maximum of 50 steps for safety and performance.\n",
    "\n",
    "- You can run this notebook as many times as you'd like to generate new electrolyte compositions and exploration paths. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b870dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from langchain_core.messages import HumanMessage\n",
    "import json\n",
    "\n",
    "\n",
    "# Load the initial prompt from file\n",
    "PROMPT_PATH = \"model_resources/prompts/initial_prompt.txt\"\n",
    "with open(PROMPT_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    initial_prompt = f.read()\n",
    "\n",
    "# Automatically generate a file name based on the current date and time\n",
    "file_name = datetime.now().strftime(\"results/results_%Y%m%d_%H%M%S.txt\")\n",
    "\n",
    "with open(file_name, 'w', encoding=\"utf-8\") as file:\n",
    "    for s in graph.stream(\n",
    "        {\n",
    "            \"messages\": [\n",
    "                HumanMessage(content=initial_prompt)\n",
    "            ],\n",
    "        },\n",
    "        {\"recursion_limit\": 50},  # Maximum number of steps to take in the graph\n",
    "    ):\n",
    "        print(s)\n",
    "        print(\"----\")\n",
    "\n",
    "        file.write(str(s) + '\\n')\n",
    "        file.write(\"----\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "electrolyte-discovery",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
